#!/usr/bin/env python3
"""
Script de teste para verificar as melhorias implementadas
"""

import json
import sys
import os

# Adicionar o diret√≥rio raiz ao path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_mock_exam_tool():
    """Testa o MockExamTool melhorado"""
    print("üéØ Testando MockExamTool...")
    
    try:
        from tools.mock_exam_tool import MockExamTool
        
        tool = MockExamTool()
        
        # Teste 1: Gerar simulado
        params = {
            "banca": "CESPE",
            "subjects": ["Portugu√™s", "Matem√°tica", "Direito"],
            "num_questions": 10,
            "difficulty": "medium",
            "cargo": "Analista Judici√°rio"
        }
        
        result = tool._run("generate_exam", json.dumps(params))
        exam_data = json.loads(result)
        
        print(f"‚úÖ Simulado gerado com {exam_data['total_questions']} quest√µes")
        print(f"   Banca: {exam_data['banca']}")
        print(f"   Tempo estimado: {exam_data['estimated_time']} minutos")
        
        # Teste 2: Avaliar simulado
        answers = {}
        for i, question in enumerate(exam_data['questions'][:3]):
            answers[question['id']] = 'A'  # Simular respostas
        
        eval_params = {
            "exam_id": exam_data['id'],
            "answers": answers
        }
        
        eval_result = tool._run("evaluate_exam", json.dumps(eval_params))
        eval_data = json.loads(eval_result)
        
        print(f"‚úÖ Avalia√ß√£o conclu√≠da - Pontua√ß√£o: {eval_data['score']}%")
        print(f"   Acertos: {eval_data['correct_count']}/{eval_data['total_questions']}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro no MockExamTool: {e}")
        return False

def test_web_search_tool():
    """Testa o WebSearchTool melhorado"""
    print("\nüîç Testando WebSearchTool...")
    
    try:
        from tools.web_search_tool import WebSearchTool
        
        tool = WebSearchTool()
        
        # Teste de busca
        params = {
            "cargo": "Analista Judici√°rio",
            "concurso": "TRF",
            "banca": "CESPE",
            "cidade": "Bras√≠lia",
            "max_results": 5
        }
        
        result = tool._run("search_exams", json.dumps(params))
        search_data = json.loads(result)
        
        print(f"‚úÖ Busca realizada - {search_data['total_results']} resultados")
        print(f"   Extra√ß√µes bem-sucedidas: {search_data['successful_extractions']}")
        print(f"   Recomenda√ß√µes: {len(search_data['recommendations'])}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro no WebSearchTool: {e}")
        return False

def test_question_api_tool():
    """Testa o QuestionAPITool melhorado"""
    print("\n‚ùì Testando QuestionAPITool...")
    
    try:
        from tools.question_api_tool import QuestionAPITool
        
        tool = QuestionAPITool()
        
        # Teste 1: Buscar quest√µes
        params = {
            "subjects": ["Portugu√™s", "Matem√°tica"],
            "difficulty": "medium",
            "count": 5,
            "banca": "CESPE"
        }
        
        result = tool._run("fetch_questions", json.dumps(params))
        questions = json.loads(result)
        
        print(f"‚úÖ {len(questions)} quest√µes obtidas")
        
        if questions:
            first_question = questions[0]
            print(f"   Primeira quest√£o: {first_question['subject']} - {first_question['difficulty']}")
        
        # Teste 2: Quiz di√°rio
        study_plan = {
            "subject_distribution": {
                "Portugu√™s": {"hours_per_week": 8},
                "Matem√°tica": {"hours_per_week": 6}
            }
        }
        
        quiz_params = {
            "study_plan": study_plan,
            "performance_history": []
        }
        
        quiz_result = tool._run("daily_quiz", json.dumps(quiz_params))
        quiz_data = json.loads(quiz_result)
        
        print(f"‚úÖ Quiz di√°rio gerado com {len(quiz_data['questions'])} quest√µes")
        print(f"   Mat√©rias de foco: {', '.join(quiz_data['focus_subjects'])}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro no QuestionAPITool: {e}")
        return False

def test_question_bank():
    """Testa o banco de quest√µes"""
    print("\nüìö Testando banco de quest√µes...")
    
    try:
        with open('data/questions/question_bank.json', 'r', encoding='utf-8') as f:
            bank = json.load(f)
        
        total_questions = 0
        for subject, questions in bank['questions'].items():
            total_questions += len(questions)
            print(f"   {subject}: {len(questions)} quest√µes")
        
        print(f"‚úÖ Banco carregado com {total_questions} quest√µes totais")
        print(f"   Mat√©rias: {len(bank['questions'])}")
        print(f"   Bancas suportadas: {', '.join(bank['metadata']['bancas'])}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro no banco de quest√µes: {e}")
        return False

def test_dashboard_data():
    """Testa os dados do dashboard"""
    print("\nüìä Testando dados do dashboard...")
    
    try:
        with open('data/dashboard/dashboard_data.json', 'r', encoding='utf-8') as f:
            dashboard = json.load(f)
        
        print(f"‚úÖ Dashboard carregado")
        print(f"   Progresso geral: {dashboard['progress_summary']['completion_percentage']}%")
        print(f"   Mat√©rias: {len(dashboard['subject_progress'])}")
        print(f"   Atividades pr√≥ximas: {len(dashboard['upcoming_activities'])}")
        print(f"   Conquistas: {len(dashboard.get('achievements', []))}")
        print(f"   Recomenda√ß√µes: {len(dashboard.get('recommendations', []))}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro nos dados do dashboard: {e}")
        return False

def test_config():
    """Testa o sistema de configura√ß√£o"""
    print("\n‚öôÔ∏è Testando configura√ß√£o...")
    
    try:
        from app.utils.config import load_config, save_config
        
        config = load_config()
        
        print(f"‚úÖ Configura√ß√£o carregada")
        print(f"   App: {config['app']['name']} v{config['app']['version']}")
        print(f"   Features ativas: {sum(1 for f in config['features'].values() if f)}")
        print(f"   Bancas suportadas: {len(config['study']['supported_bancas'])}")
        print(f"   Mat√©rias suportadas: {len(config['study']['supported_subjects'])}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro na configura√ß√£o: {e}")
        return False

def test_gamification_system():
    """Testa o sistema de gamifica√ß√£o"""
    print("\nüéÆ Testando sistema de gamifica√ß√£o...")

    try:
        from app.utils.gamification import GamificationSystem

        # Criar sistema de gamifica√ß√£o
        gamification = GamificationSystem("test_user")

        # Testar adi√ß√£o de experi√™ncia
        result = gamification.add_experience(100, "daily_quiz")
        print(f"‚úÖ Experi√™ncia adicionada: {result}")

        # Testar verifica√ß√£o de conquistas
        activity_data = {
            "current_streak": 7,
            "study_hours": 25,
            "best_score": 85
        }

        new_achievements = gamification.check_achievements(activity_data)
        print(f"‚úÖ Conquistas verificadas: {len(new_achievements)} novas")

        # Testar resumo do usu√°rio
        summary = gamification.get_user_summary()
        print(f"‚úÖ Resumo gerado - N√≠vel: {summary['level']}, Pontos: {summary['total_points']}")

        return True

    except Exception as e:
        print(f"‚ùå Erro no sistema de gamifica√ß√£o: {e}")
        return False

def test_performance_predictor():
    """Testa o sistema de predi√ß√£o de desempenho"""
    print("\nüîÆ Testando preditor de desempenho...")

    try:
        from app.utils.performance_predictor import PerformancePredictor

        predictor = PerformancePredictor()

        # Dados de teste
        user_data = {
            'mock_exam_scores': [
                {'score': 65, 'date': '2024-01-01'},
                {'score': 70, 'date': '2024-01-08'},
                {'score': 75, 'date': '2024-01-15'}
            ],
            'subject_progress': {
                'Portugu√™s': {'last_score': 78},
                'Matem√°tica': {'last_score': 65},
                'Direito': {'last_score': 82}
            },
            'total_study_hours': 80,
            'simulados_completed': 3
        }

        # Testar an√°lise de desempenho
        metrics = predictor.analyze_performance(user_data)
        print(f"‚úÖ M√©tricas analisadas - Score geral: {metrics.overall_score:.1f}%")

        # Testar predi√ß√£o
        prediction = predictor.predict_exam_performance(user_data, "CESPE", 90)
        print(f"‚úÖ Predi√ß√£o gerada - Score previsto: {prediction.predicted_score:.1f}%")
        print(f"   Confian√ßa: {prediction.confidence:.1f}%")

        return True

    except Exception as e:
        print(f"‚ùå Erro no preditor de desempenho: {e}")
        return False

def test_notifications_system():
    """Testa o sistema de notifica√ß√µes"""
    print("\nüîî Testando sistema de notifica√ß√µes...")

    try:
        from app.utils.notifications import NotificationManager, NotificationType, NotificationPriority

        # Criar gerenciador de notifica√ß√µes
        manager = NotificationManager("test_user")

        # Criar notifica√ß√£o de teste
        notification = manager.create_notification(
            NotificationType.STUDY_REMINDER,
            "Teste de Notifica√ß√£o",
            "Esta √© uma notifica√ß√£o de teste",
            NotificationPriority.MEDIUM
        )

        print(f"‚úÖ Notifica√ß√£o criada: {notification.title}")

        # Testar busca de notifica√ß√µes n√£o lidas
        unread = manager.get_unread_notifications()
        print(f"‚úÖ Notifica√ß√µes n√£o lidas: {len(unread)}")

        # Testar resumo
        summary = manager.get_notification_summary()
        print(f"‚úÖ Resumo gerado - Total n√£o lidas: {summary['total_unread']}")

        return True

    except Exception as e:
        print(f"‚ùå Erro no sistema de notifica√ß√µes: {e}")
        return False

def test_writing_system():
    """Testa o sistema avan√ßado de reda√ß√£o"""
    print("\n‚úçÔ∏è Testando sistema de reda√ß√£o...")

    try:
        from tools.writing_tool import WritingTool

        # Criar ferramenta de reda√ß√£o
        writing_tool = WritingTool()

        # Texto de teste
        texto_teste = """
        A sustentabilidade ambiental representa um dos maiores desafios contempor√¢neos.

        Em primeiro lugar, √© fundamental reconhecer que o desenvolvimento econ√¥mico n√£o pode ocorrer em detrimento do meio ambiente. Portanto, torna-se necess√°rio implementar pol√≠ticas p√∫blicas que promovam o equil√≠brio entre crescimento e preserva√ß√£o.

        Ademais, a educa√ß√£o ambiental constitui ferramenta essencial para conscientiza√ß√£o da popula√ß√£o. Assim, investimentos em programas educacionais podem gerar mudan√ßas significativas no comportamento social.

        Conclui-se, portanto, que a sustentabilidade ambiental exige a√ß√µes coordenadas entre governo, empresas e sociedade civil para garantir um futuro sustent√°vel para as pr√≥ximas gera√ß√µes.
        """

        # Testar avalia√ß√£o por banca
        resultado = writing_tool.evaluate_essay_by_banca(
            texto_teste,
            "CESPE",
            "dissertativo-argumentativo",
            "Sustentabilidade ambiental"
        )

        print(f"‚úÖ Reda√ß√£o avaliada - Nota: {resultado['score_final']}/10")
        print(f"   Banca: {resultado['banca']}")
        print(f"   Crit√©rios avaliados: {len(resultado['scores_por_criterio'])}")

        # Testar busca de tema
        tema = writing_tool.get_tema_by_banca("CESPE")
        print(f"‚úÖ Tema obtido: {tema.get('tema', 'Erro')}")

        # Testar informa√ß√µes da banca
        info_banca = writing_tool.banca_patterns["FCC"]
        print(f"‚úÖ Padr√µes FCC carregados - Caracter√≠sticas: {len(info_banca['caracteristicas'])}")

        return True

    except Exception as e:
        print(f"‚ùå Erro no sistema de reda√ß√£o: {e}")
        return False

def main():
    """Executa todos os testes"""
    print("üöÄ Iniciando testes das melhorias implementadas...\n")

    tests = [
        test_config,
        test_question_bank,
        test_dashboard_data,
        test_mock_exam_tool,
        test_web_search_tool,
        test_question_api_tool,
        test_gamification_system,
        test_performance_predictor,
        test_notifications_system,
        test_writing_system
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
    
    print(f"\nüìã Resumo dos testes:")
    print(f"   ‚úÖ Passou: {passed}/{total}")
    print(f"   ‚ùå Falhou: {total - passed}/{total}")
    
    if passed == total:
        print("\nüéâ Todos os testes passaram! Sistema funcionando corretamente.")
        return 0
    else:
        print(f"\n‚ö†Ô∏è {total - passed} teste(s) falharam. Verifique os erros acima.")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
